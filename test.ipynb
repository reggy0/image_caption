{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP46HhGgkLfSozpykH28C0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reggy0/image_caption/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q0km4cUztHV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pickle import load\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from utils.model import CNNModel, generate_caption_beam_search\n",
        "import os\n",
        "\n",
        "from config import config\n",
        "\n",
        "\"\"\"\n",
        "    *Some simple checking\n",
        "\"\"\"\n",
        "assert type(config['max_length']) is int, 'Please provide an integer value for `max_length` parameter in config.py file'\n",
        "assert type(config['beam_search_k']) is int, 'Please provide an integer value for `beam_search_k` parameter in config.py file'\n",
        "\n",
        "# Extract features from each image in the directory\n",
        "def extract_features(filename, model, model_type):\n",
        "\tif model_type == 'inceptionv3':\n",
        "\t\tfrom keras.applications.inception_v3 import preprocess_input\n",
        "\t\ttarget_size = (299, 299)\n",
        "\telif model_type == 'vgg16':\n",
        "\t\tfrom keras.applications.vgg16 import preprocess_input\n",
        "\t\ttarget_size = (224, 224)\n",
        "\t# Loading and resizing image\n",
        "\timage = load_img(filename, target_size=target_size)\n",
        "\t# Convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# Reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# Prepare the image for the CNN Model model\n",
        "\timage = preprocess_input(image)\n",
        "\t# Pass image into model to get encoded features\n",
        "\tfeatures = model.predict(image, verbose=0)\n",
        "\treturn features\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_path = config['tokenizer_path']\n",
        "tokenizer = load(open(tokenizer_path, 'rb'))\n",
        "\n",
        "# Max sequence length (from training)\n",
        "max_length = config['max_length']\n",
        "\n",
        "# Load the model\n",
        "caption_model = load_model(config['model_load_path'])\n",
        "\n",
        "image_model = CNNModel(config['model_type'])\n",
        "\n",
        "# Load and prepare the image\n",
        "for image_file in os.listdir(config['test_data_path']):\n",
        "\tif(image_file.split('--')[0]=='output'):\n",
        "\t\tcontinue\n",
        "\tif(image_file.split('.')[1]=='jpg' or image_file.split('.')[1]=='jpeg'):\n",
        "\t\tprint('Generating caption for {}'.format(image_file))\n",
        "\t\t# Encode image using CNN Model\n",
        "\t\timage = extract_features(config['test_data_path']+image_file, image_model, config['model_type'])\n",
        "\t\t# Generate caption using Decoder RNN Model + BEAM search\n",
        "\t\tgenerated_caption = generate_caption_beam_search(caption_model, tokenizer, image, max_length, beam_index=config['beam_search_k'])\n",
        "\t\t# Remove startseq and endseq\n",
        "\t\tcaption = 'Caption: ' + generated_caption.split()[1].capitalize()\n",
        "\t\tfor x in generated_caption.split()[2:len(generated_caption.split())-1]:\n",
        "\t\t    caption = caption + ' ' + x\n",
        "\t\tcaption += '.'\n",
        "\t\t# Show image and its caption\n",
        "\t\tpil_im = Image.open(config['test_data_path']+image_file, 'r')\n",
        "\t\tfig, ax = plt.subplots(figsize=(8, 8))\n",
        "\t\tax.get_xaxis().set_visible(False)\n",
        "\t\tax.get_yaxis().set_visible(False)\n",
        "\t\t_ = ax.imshow(np.asarray(pil_im), interpolation='nearest')\n",
        "\t\t_ = ax.set_title(\"BEAM Search with k={}\\n{}\".format(config['beam_search_k'],caption),fontdict={'fontsize': '20','fontweight' : '40'})\n",
        "\t\tplt.savefig(config['test_data_path']+'output--'+image_file)"
      ]
    }
  ]
}